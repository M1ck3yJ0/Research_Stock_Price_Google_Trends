# Data Collection

## Generation of Gen Weekly Trends URLs

## Generation of Gen All-Time Trends URLs

import pandas as pd
import urllib.parse
from google.colab import files

# List of keywords
keywords = [
  "Election", "Congress", "Legislation", "Policy", "Regulation",
  "War", "Conflict", "Treaty", "Agreement", "Recession",
  "Inflation", "Unemployment", "GDP", "Rates", "Economy",
  "Market", "Investment", "Debt", "Currency", "Interest",
  "Sustainable", "Sustainability", "ESG", "Climate", "Carbon",
  "Renewable", "Diversity", "Ethics", "Governance", "Emissions",
  "Green", "Environmental", "Waste", "Labor", "Innovation",
  "Disruption", "Technology", "Digital", "AI", "Data",
  "Growth", "Crisis", "Scandal", "Bankruptcy", "Lawsuit",
  "Sales", "Cost", "Risk", "Confidence", "Uncertainty",
  "Recovery", "Layoffs", "Trade", "Tariffs", "Energy",
  "Supply", "Shortage", "Sanctions", "Pandemic", "Stocks",
  "Shares", "Investing", "Portfolio", "Dividends", "Earnings",
  "Revenue", "Profit", "Loss", "Rally", "Crash",
  "Bear", "Bull", "Liquidity", "Volatility", "Fund",
  "Merger", "Acquisition", "Valuation", "Downgrade", "Upgrade"
]

# List of countries/regions
regions = ["US", "GB", "IN", "SG", "AU"]
regions_param = ",".join(regions)

# Generate URLs for each keyword
data = []
for keyword in keywords:
    # Create the q parameter with the keyword repeated for each region
    q_param = ",".join([keyword] * 5)  # Repeat the keyword 5 times

    # Construct the URL
    url = f"https://trends.google.com/trends/explore?date=all,all,all,all,all&geo={regions_param}&q={q_param}&hl=en"

    data.append({"keyword": keyword, "url": url})

# Create a DataFrame
df = pd.DataFrame(data)

# Preview the first few URLs
print("Preview of generated URLs (first 5):")
for i in range(min(5, len(data))):
    print(f"\nKeyword: {data[i]['keyword']}")
    print(f"URL: {data[i]['url']}")

print(f"\nTotal URLs generated: {len(data)}")

# Save to CSV and download
csv_filename = "google_trends_urls.csv"
df.to_csv(csv_filename, index=False)
files.download(csv_filename)

print(f"\nCSV file '{csv_filename}' has been created and downloaded.")

## Generation of Company Trends URLs

import pandas as pd
import csv
from io import StringIO
import re

# Define company data - using a list of lists to avoid CSV parsing issues
company_data = """Name,Ticker
Hindustan Zinc Limited,HINDZINC
Mahindra & Mahindra Financial Services Ltd.,M&MFIN
Mahindra & Mahindra Ltd.,M&M
Tata Communications Limited,TATACOMM
Tech Mahindra Limited,TECHM
Bharat Petroleum Corporation Limited,BPCL
Ashok Leyland Limited,ASHOKLEY
Steel Authority of India Limited,SAIL
Vodafone Idea Ltd,IDEA
Zydus Lifesciences Limited,ZYDUSLIFE
Godrej Industries Limited,GODREJIND
Power Finance Corporation Limited,PFC
Bharat Heavy Electricals Limited,BHEL
Punjab National Bank,PNB
Mangalore Refinery & Petrochemicals Ltd.,MRPL
Coca-Cola HBC AG,CCH
Barclays PLC,BARC
Kingfisher Plc,KGF
Jupiter Fund Management plc,JUP
Serco Group plc,SRP
Chemring Group PLC,CHG
easyJet plc,EZJ
Fresnillo PLC,FRES
Close Brothers Group plc,CBG
Grainger plc,GRI
ME Group International plc,MEGP
Clarkson PLC,CKN
Schroder Real Estate Investment Trust Ltd,SREI
S&U plc,SUS
Kier Group plc,KIE
Johnson & Johnson,JNJ
Coca-Cola Europacific Partners plc,CCEP
Hewlett Packard Enterprise Co.,HPE
Intel Corporation,INTC
HP Inc.,HPQ
Honeywell International Inc.,HON
Halliburton Company,HAL
UnitedHealth Group Incorporated,UNH
Cincinnati Financial Corporation,CINF
Walt Disney Company,DIS
BioMarin Pharmaceutical Inc.,BMRN
Lennar Corporation Class A,LEN
NVR Inc.,NVR
O'Reilly Automotive Inc.,ORLY
Berkshire Hathaway Inc. Class A,BRK.A"""

# Parse the data manually
rows = company_data.strip().split('\n')
header = rows[0].split(',')
data = []

for row in rows[1:]:
    # Find the last comma to split correctly
    last_comma_pos = row.rfind(',')
    name = row[:last_comma_pos].strip()
    ticker = row[last_comma_pos+1:].strip()
    data.append([name, ticker])

# Create dataframe
df = pd.DataFrame(data, columns=header)

# Clean company names to remove Ltd, Limited, etc.
def clean_company_name(name):
    # Remove common company suffixes and jargon
    patterns = [
        r'\sLimited$', r'\sLtd\.?$', r'\sInc\.?$', r'\sCorporation$',
        r'\sCorp\.?$', r'\sAG$', r'\sPLC$', r'\splc$', r'\sClass\s[A-Z]$',
        r'\sCo\.?$', r'\s&\sCo\.?$', r',.*$',    # Remove everything after a comma
        r'\s+Ltd\.?$'  # Catch Ltd with spaces before
    ]

    cleaned = name
    for pattern in patterns:
        cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)

    # Remove trailing periods, commas, and whitespace
    cleaned = cleaned.strip(' .,')

    return cleaned

# Apply cleaning to company names
df['Clean_Name'] = df['Name'].apply(clean_company_name)

# Define years
years = list(range(2020, 2025))

# Function to create URL for a term and year
def create_url(term, year):
    start_date = f"{year}-01-01"
    end_date = f"{year}-12-31"

    # URL pattern for Google Trends with 5 regions
    term_encoded = term.replace(' ', '%20').replace('&', '%26')
    return f"https://trends.google.com/trends/explore?date={start_date}%20{end_date},{start_date}%20{end_date},{start_date}%20{end_date},{start_date}%20{end_date},{start_date}%20{end_date}&geo=US,GB,IN,SG,AU&q={term_encoded},{term_encoded},{term_encoded},{term_encoded},{term_encoded}&hl=en"

# Create a list to store all URL data
url_data = []

# Generate URLs for each company name and year
for index, row in df.iterrows():
    company_name = row['Clean_Name']
    ticker = row['Ticker']

    # Generate URLs for company name
    for year in years:
        url_data.append({
            'term': company_name,
            'type': 'Company Name',
            'year': year,
            'url': create_url(company_name, year)
        })

    # Generate URLs for ticker
    for year in years:
        url_data.append({
            'term': ticker,
            'type': 'Ticker',
            'year': year,
            'url': create_url(ticker, year)
        })

# Create a DataFrame with the URL data
url_df = pd.DataFrame(url_data)

# Save to CSV
url_df.to_csv('company_trends_urls.csv', index=False)

# Display statistics
print(f"Total companies: {len(df)}")
print(f"Total URLs generated: {len(url_df)}")
print(f"Company name URLs: {len(url_df[url_df['type'] == 'Company Name'])}")
print(f"Ticker URLs: {len(url_df[url_df['type'] == 'Ticker'])}")

# Display sample of cleaned company names for verification
print("\nSample of cleaned company names:")
for index, row in df.head(10).iterrows():
    print(f"Original: {row['Name']}")
    print(f"Cleaned: {row['Clean_Name']}")
    print("---")

# Display sample URLs to verify
print("\nSample URLs:")
for index, row in url_df.sample(5).iterrows():
    print(f"Term: {row['term']} ({row['type']}), Year: {row['year']}")
    print(f"URL: {row['url']}")
    print("---")

# For Colab, add this to download the file
from google.colab import files
files.download('company_trends_urls.csv')

# Data Preparation

## Cleaning Gen Weekly Trends Data

#!/usr/bin/env python3
# CSV Cleaner for Google Colab - This script transforms your combined CSV file by:
# 1. Removing blank rows
# 2. Handling special dates (12/27/20, 12/26/21, 12/31/23) that come before blank lines separately
# 3. Organizing data with keywords as columns (one per country)
# 4. Creating a wide format with dates as rows and keyword-country pairs as columns

import re
import os
import csv
from collections import defaultdict
from datetime import datetime
from google.colab import files

def clean_csv(input_file, output_file):
    """
    Transform a combined CSV file into a wide format with dates as rows and
    keyword-country pairs as columns. Special handling for specific dates.

    Args:
        input_file (str): Path to the input CSV file
        output_file (str): Path to save the transformed CSV file

    Returns:
        dict: Statistics about the transformation process
    """
    try:
        # Read the input file
        with open(input_file, 'r', encoding='utf-8') as f:
            lines = f.read().splitlines()

        # Track keywords, countries, and data
        keywords = []
        countries = ["United States", "United Kingdom", "India", "Singapore", "Australia"]
        data_by_keyword_country = {}
        dates_set = set()

        # Track special dates (those that come just before a "Category: All categories" line)
        special_dates = ['2020-12-27', '2021-12-26', '2022-12-25', '2023-12-31']
        special_date_data = {}  # Will store {date: {keyword-country: value}}

        current_keyword = None

        # First pass: Extract keywords and collect data
        for i, line in enumerate(lines):
            line = line.strip()

            if line == '':
                # Skip blank lines for regular processing
                continue
            elif 'Category:' in line:
                # Check if the previous line had a special date
                if i > 0 and lines[i-1].strip() != '':
                    prev_line = lines[i-1].strip()
                    if re.match(r'^\d{4}-\d{2}-\d{2}', prev_line):
                        parts = prev_line.split(',')
                        date = parts[0]
                        if date in special_dates and current_keyword:
                            # This is a special date line just before "Category: All categories"
                            if date not in special_date_data:
                                special_date_data[date] = {}

                            # Store values for each country
                            for j, country in enumerate(countries):
                                country_idx = j + 1  # Index in the CSV row (1-5)
                                if country_idx < len(parts):
                                    key = f"{current_keyword} ({country})"
                                    try:
                                        special_date_data[date][key] = float(parts[country_idx])
                                    except ValueError:
                                        special_date_data[date][key] = parts[country_idx]

                # Skip category lines for regular processing
                continue
            elif line.startswith('Week,'):
                # Extract keyword from header line
                parts = line.split(',')
                if len(parts) > 1:
                    first_part = parts[1]  # e.g., "Growth: (United States)"
                    keyword_match = re.search(r'([^:]+):', first_part)
                    if keyword_match:
                        current_keyword = keyword_match.group(1).strip()
                        if current_keyword not in keywords:
                            keywords.append(current_keyword)
            elif re.match(r'^\d{4}-\d{2}-\d{2}', line) and current_keyword:
                # Process data row
                parts = line.split(',')
                if len(parts) >= 6:  # Ensure we have enough parts
                    date = parts[0]

                    # Skip if this is a special date line that will be handled separately
                    # (We'll still process it for the regular output, but also track it specially)
                    if date in special_dates:
                        # Check if next line contains "Category: All categories"
                        next_line_category = (i+1 < len(lines) and 'Category:' in lines[i+1])
                        if next_line_category:
                            continue  # Skip for regular processing, it's handled above

                    dates_set.add(date)

                    # Store values for each country
                    for j, country in enumerate(countries):
                        country_idx = j + 1  # Index in the CSV row (1-5)
                        if country_idx < len(parts):
                            key = f"{current_keyword} ({country})"
                            if key not in data_by_keyword_country:
                                data_by_keyword_country[key] = {}

                            # Store the value for this date
                            try:
                                data_by_keyword_country[key][date] = float(parts[country_idx])
                            except ValueError:
                                # Handle non-numeric values
                                data_by_keyword_country[key][date] = parts[country_idx]

        # Sort dates chronologically
        all_dates = sorted(list(dates_set), key=lambda x: datetime.strptime(x, '%Y-%m-%d'))

        # Create column headers (Week + all keyword-country combinations)
        headers = ["Week"]
        for keyword in keywords:
            for country in countries:
                headers.append(f"{keyword} ({country})")

        # Create the output rows
        output_rows = [headers]

        # First add the special dates in a separate section
        special_dates_sorted = sorted(special_date_data.keys(),
                                      key=lambda x: datetime.strptime(x, '%Y-%m-%d'))

        # Add a header for special dates section
        output_rows.append(["*** Special dates that come before 'Category: All categories' lines ***"])

        # Add each special date
        for date in special_dates_sorted:
            row = [date]
            # Add data for each keyword-country combination
            for keyword in keywords:
                for country in countries:
                    key = f"{keyword} ({country})"
                    value = special_date_data.get(date, {}).get(key, "")
                    row.append(str(value))
            output_rows.append(row)

        # Add a separator
        output_rows.append(["*** Regular dates ***"])

        # Now add all regular dates
        for date in all_dates:
            row = [date]
            # Add data for each keyword-country combination
            for keyword in keywords:
                for country in countries:
                    key = f"{keyword} ({country})"
                    value = data_by_keyword_country.get(key, {}).get(date, "")
                    row.append(str(value))
            output_rows.append(row)

        # Write to output file
        with open(output_file, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerows(output_rows)

        print(f"Original CSV had {len(lines)} lines")
        print(f"Transformed CSV has {len(output_rows)} rows")
        print(f"Created {len(headers) - 1} keyword-country columns")
        print(f"Special dates found: {len(special_date_data)}")
        print(f"File successfully saved to {output_file}")

        # Download the file automatically in Colab
        try:
            files.download(output_file)
            print(f"File download initiated for {output_file}")
        except:
            print("Note: To download the file in Colab, you may need to run this in a cell with '!python script.py'")

        return {
            "original_line_count": len(lines),
            "transformed_row_count": len(output_rows),
            "column_count": len(headers),
            "keyword_count": len(keywords),
            "date_count": len(all_dates),
            "special_dates_count": len(special_date_data)
        }

    except Exception as e:
        print(f"Error processing CSV: {str(e)}")
        return None

# Use this in a Colab notebook as follows:
# 1. Upload your combined.csv file to Colab
# 2. Run this cell with the following code (uncomment and replace with your actual filenames)

input_file = "/content/Trends_combined_RAW.csv"  # The name of your uploaded file
output_file = "transformed_Trends.csv"  # The name you want for the transformed file
clean_csv(input_file, output_file)

## 
